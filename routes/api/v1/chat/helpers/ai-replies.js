/**
 * AI Quick Reply Generation Helper
 * Generates contextual quick reply suggestions using Groq API
 * @module chat/helpers/ai-replies
 */

const logger = require('../../../../../utils/logger');

const GROQ_API_KEY = process.env.GROQ_API_KEY;
const GROQ_API_URL = 'https://api.groq.com/openai/v1/chat/completions';

/**
 * Generate AI-powered quick reply suggestions
 * @param {string} lastMessage - The last message from support
 * @param {Array} conversationContext - Recent conversation history
 * @returns {Promise<Array<string>|null>} Array of quick reply suggestions
 */
async function generateAIQuickReplies(lastMessage, conversationContext = []) {
  if (!GROQ_API_KEY || !lastMessage) {
    return null;
  }

  try {
    // Build context from recent messages
    const contextMessages = conversationContext.slice(-5).map(m =>
      `${m.sender === 'candidate' ? 'Worker' : 'Support'}: ${m.content}`
    ).join('\n');

    const prompt = `You are helping a gig worker generate quick reply suggestions for a chat with their employer/support team.

Recent conversation:
${contextMessages}

Last message from support: "${lastMessage}"

Generate exactly 3-4 short, natural reply options the worker might want to send. Each reply should be:
- Brief (2-6 words max)
- Natural and conversational
- Relevant to the last message
- Different from each other (cover different intents like confirm, ask question, decline, etc.)

If the support is asking about jobs/shifts, include options like availability confirmation.
If asking about payments, include acknowledgment options.
If asking a yes/no question, include both yes and no options.
If it's a greeting, include friendly responses.

Return ONLY a JSON array of strings, nothing else. Example: ["Yes, I can", "What time?", "Not available", "Tell me more"]`;

    const response = await fetch(GROQ_API_URL, {
      method: 'POST',
      headers: {
        'Authorization': `Bearer ${GROQ_API_KEY}`,
        'Content-Type': 'application/json',
      },
      body: JSON.stringify({
        model: 'llama-3.1-8b-instant',
        messages: [
          {
            role: 'system',
            content: 'You are a helpful assistant that generates quick reply suggestions for mobile chat interfaces. Always respond with valid JSON arrays only.'
          },
          {
            role: 'user',
            content: prompt
          }
        ],
        temperature: 0.7,
        max_tokens: 100,
      }),
    });

    if (!response.ok) {
      logger.warn('Groq API request failed', {
        status: response.status,
        statusText: response.statusText
      });
      return getDefaultReplies(lastMessage);
    }

    const data = await response.json();
    const generatedContent = data.choices?.[0]?.message?.content?.trim();

    if (!generatedContent) {
      logger.warn('No content generated by Groq API');
      return getDefaultReplies(lastMessage);
    }

    // Parse the JSON response
    try {
      const replies = JSON.parse(generatedContent);
      if (Array.isArray(replies) && replies.length > 0) {
        // Validate and clean replies
        const cleanReplies = replies
          .filter(reply => typeof reply === 'string' && reply.trim().length > 0)
          .map(reply => reply.trim())
          .slice(0, 4); // Max 4 replies

        if (cleanReplies.length > 0) {
          return cleanReplies;
        }
      }
    } catch (parseError) {
      logger.warn('Failed to parse Groq API response as JSON', {
        content: generatedContent,
        error: parseError.message
      });
    }

    // If parsing fails, return default replies
    return getDefaultReplies(lastMessage);

  } catch (error) {
    logger.error('Error generating AI quick replies', {
      error: error.message,
      lastMessage: lastMessage?.substring(0, 100)
    });
    return getDefaultReplies(lastMessage);
  }
}

/**
 * Get default quick reply suggestions based on message content
 * @param {string} message - The message to analyze
 * @returns {Array<string>} Default reply suggestions
 */
function getDefaultReplies(message = '') {
  const lowerMessage = message.toLowerCase();

  // Pattern-based default replies
  if (lowerMessage.includes('job') || lowerMessage.includes('shift') || lowerMessage.includes('work')) {
    return ['Yes, I can', 'When?', 'Not available', 'Tell me more'];
  }

  if (lowerMessage.includes('payment') || lowerMessage.includes('pay') || lowerMessage.includes('money')) {
    return ['Got it', 'When?', 'How much?', 'Thank you'];
  }

  if (lowerMessage.includes('?')) {
    return ['Yes', 'No', 'Maybe', 'Let me check'];
  }

  if (lowerMessage.includes('hello') || lowerMessage.includes('hi') || lowerMessage.includes('morning')) {
    return ['Hello!', 'Good morning', 'How are you?', 'Thank you'];
  }

  if (lowerMessage.includes('interview') || lowerMessage.includes('schedule')) {
    return ['Available', 'What time?', 'Not today', 'Reschedule?'];
  }

  // General default replies
  return ['Okay', 'Yes', 'No', 'Thank you'];
}

/**
 * Generate contextual quick replies for specific scenarios
 * @param {string} scenario - The chat scenario (job_offer, payment_inquiry, etc.)
 * @param {Object} context - Additional context data
 * @returns {Array<string>} Scenario-specific replies
 */
function getScenarioReplies(scenario, context = {}) {
  const scenarios = {
    job_offer: ['Accept', 'Decline', 'When?', 'More details?'],
    payment_inquiry: ['Received', 'Not yet', 'When?', 'Check again'],
    interview_scheduling: ['Available', 'Busy then', 'What time?', 'Tomorrow?'],
    general_question: ['Yes', 'No', 'Maybe', 'Tell me more'],
    greeting: ['Hello!', 'Good day!', 'Thank you', 'How are you?'],
    urgent_request: ['On it', 'Need time', 'Available', 'Call me'],
    feedback_request: ['Good', 'Needs work', 'Excellent', 'Will improve']
  };

  return scenarios[scenario] || getDefaultReplies();
}

module.exports = {
  generateAIQuickReplies,
  getDefaultReplies,
  getScenarioReplies
};